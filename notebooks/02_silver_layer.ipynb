{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6790b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eeb89a",
   "metadata": {},
   "source": [
    "--- Execução do Notebook Silver ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abadc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo Bronze\n",
    "bronze_file_path = '../data/bronze/dados_brutos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76db3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados originais carregados: (325753, 31)\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados da camada Bronze\n",
    "try:\n",
    "    if not os.path.exists(bronze_file_path):\n",
    "        print(f\"Erro: Arquivo '{bronze_file_path}' não encontrado.\")\n",
    "        df_silver = pd.DataFrame()\n",
    "    else:\n",
    "        df_silver = pd.read_csv(bronze_file_path)\n",
    "        print(f\"Dados originais carregados: {df_silver.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao carregar o arquivo '{bronze_file_path}': {e}\")\n",
    "    df_silver = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63a03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatas removidas: 0\n",
      "Tratamento de nulos concluído.\n",
      "Colunas de texto padronizadas (strip/upper).\n",
      "Renomeada a coluna 'District ' para 'District'.\n",
      "Removida 0 linha(s) de 'header como dado'.\n",
      "Coluna 'State District' criada com sucesso.\n",
      "Colunas 'State' e 'District' removidas.\n",
      "Dados filtrados para as colheitas desejadas. Novo shape: (56086, 30)\n",
      "\n",
      "Removendo registros com 'Area' ou 'Yield' zerados...\n",
      "Linhas removidas pelos filtros de zero: 404\n",
      "Linhas restantes após o filtro de zeros: 55682\n",
      "\n",
      "=== VALIDAÇÕES DE QUALIDADE ===\n",
      "Completude: 100.00%\n",
      "Unicidade: 100.00% (Total de 0 duplicatas restantes)\n",
      "\n",
      "Dados limpos salvos: (55682, 31)\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o DataFrame não está vazio antes de prosseguir com as transformações\n",
    "if not df_silver.empty:\n",
    "    # ==========================================\n",
    "    # TRANSFORMAÇÃO 1: Remover Duplicatas\n",
    "    # ==========================================\n",
    "    linhas_antes = len(df_silver)\n",
    "    df_silver = df_silver.drop_duplicates().copy()\n",
    "    linhas_depois = len(df_silver)\n",
    "    print(f\"Duplicatas removidas: {linhas_antes - linhas_depois}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TRANSFORMAÇÃO 2: Tratar Valores Nulos\n",
    "    # ==========================================\n",
    "    colunas_regiao = ['State', 'District ']\n",
    "    df_silver = df_silver.dropna(subset=colunas_regiao).copy()\n",
    "\n",
    "    # Preencher valores numéricos com a mediana\n",
    "    colunas_numericas = df_silver.select_dtypes(include=[np.number]).columns\n",
    "    df_silver[colunas_numericas] = df_silver[colunas_numericas].fillna(df_silver[colunas_numericas].median())\n",
    "\n",
    "    # Preencher valores categóricos restantes com 'Desconhecido'\n",
    "    colunas_texto = df_silver.select_dtypes(include=['object']).columns\n",
    "    df_silver[colunas_texto] = df_silver[colunas_texto].fillna('Desconhecido')\n",
    "    print(f\"Tratamento de nulos concluído.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TRANSFORMAÇÃO 3: Padronizar Valores E CRIAR COLUNA-CHAVE (MELHORIA 2)\n",
    "    # ==========================================\n",
    "    # 1. Padronizar colunas de texto (strip/upper)\n",
    "    colunas_para_padronizar = ['State', 'District ', 'Crop', 'Season']\n",
    "    for coluna in colunas_para_padronizar:\n",
    "        if coluna in df_silver.columns:\n",
    "            # Padroniza antes do filtro de cultura\n",
    "            df_silver[coluna] = df_silver[coluna].astype(str).str.strip().str.upper().copy()\n",
    "    print(\"Colunas de texto padronizadas (strip/upper).\")\n",
    "\n",
    "    # 2. Renomear 'District ' (com espaço) para 'District' (sem espaço)\n",
    "    if 'District ' in df_silver.columns:\n",
    "        df_silver.rename(columns={'District ': 'District'}, inplace=True)\n",
    "        print(\"Renomeada a coluna 'District ' para 'District'.\")\n",
    "\n",
    "    # Filtra linhas onde a coluna 'State' (agora em maiúsculo) tem o valor literal 'STATE'.\n",
    "    linhas_antes_filtro_header = len(df_silver)\n",
    "    df_silver = df_silver[df_silver['State'] != 'STATE'].copy()\n",
    "    print(f\"Removida {linhas_antes_filtro_header - len(df_silver)} linha(s) de 'header como dado'.\")\n",
    "\n",
    "    # 3. Criar a coluna combinada 'State District'\n",
    "    if 'State' in df_silver.columns and 'District' in df_silver.columns:\n",
    "        df_silver['State District'] = (df_silver['State'] + ' - ' + df_silver['District']).copy()\n",
    "        print(\"Coluna 'State District' criada com sucesso.\")\n",
    "\n",
    "    # Remover colunas State e District\n",
    "    df_silver.drop(columns=['State', 'District'], inplace=True)\n",
    "    print(\"Colunas 'State' e 'District' removidas.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TRANSFORMAÇÃO 4: Simplificando o escopo de colheita/Crop (Filtro)\n",
    "    # ==========================================\n",
    "    # Agora o filtro é aplicado APÓS a padronização para UPPER()\n",
    "    valores_desejados = ['MAIZE','RICE','WHEAT','BARLEY']\n",
    "    if 'Crop' in df_silver.columns:\n",
    "        mascara = df_silver['Crop'].isin(valores_desejados)\n",
    "        df_silver = df_silver[mascara].copy()\n",
    "        print(f\"Dados filtrados para as colheitas desejadas. Novo shape: {df_silver.shape}\")\n",
    "    else:\n",
    "        print(\"Aviso: A coluna 'Crop' não foi encontrada. A Transformação 4 foi ignorada.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TRANSFORMAÇÃO 5: Remover Registros de Produção Zero (e Yield Zero)\n",
    "    # ==========================================\n",
    "    print(f\"\\nRemovendo registros com 'Area' ou 'Yield' zerados...\")\n",
    "    linhas_antes_filtro_zero = len(df_silver)\n",
    "\n",
    "    # Filtro 1: Remove registros onde a área plantada é 0 (dado inválido)\n",
    "    if 'Area' in df_silver.columns:\n",
    "        df_silver = df_silver[df_silver['Area'] > 0].copy()\n",
    "\n",
    "    # Filtro 2: Remove registros onde o rendimento é 0 (dado inútil para análise)\n",
    "    if 'Yield' in df_silver.columns:\n",
    "        df_silver = df_silver[df_silver['Yield'] > 0].copy()\n",
    "\n",
    "    linhas_depois_filtro_zero = len(df_silver)\n",
    "    print(f\"Linhas removidas pelos filtros de zero: {linhas_antes_filtro_zero - linhas_depois_filtro_zero}\")\n",
    "    print(f\"Linhas restantes após o filtro de zeros: {len(df_silver)}\")\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # VALIDAÇÕES BÁSICAS (Para fins de log)\n",
    "    # ==========================================\n",
    "    print(\"\\n=== VALIDAÇÕES DE QUALIDADE ===\")\n",
    "    if not df_silver.empty:\n",
    "        total_celulas = df_silver.shape[0] * df_silver.shape[1]\n",
    "        celulas_preenchidas = df_silver.count().sum()\n",
    "        completude = (celulas_preenchidas / total_celulas) * 100\n",
    "        print(f\"Completude: {completude:.2f}%\")\n",
    "\n",
    "        duplicatas = df_silver.duplicated().sum()\n",
    "        unicidade = ((len(df_silver) - duplicatas) / len(df_silver)) * 100\n",
    "        print(f\"Unicidade: {unicidade:.2f}% (Total de {duplicatas} duplicatas restantes)\")\n",
    "    else:\n",
    "        print(\"Validação ignorada: DataFrame está vazio após as transformações.\")\n",
    "\n",
    "    # Adicionar informações de processamento\n",
    "    df_silver['data_processamento'] = datetime.now()\n",
    "\n",
    "    # Salvar na camada Silver\n",
    "    silver_file_path = '../data/silver/dados_limpos.csv'\n",
    "    os.makedirs(os.path.dirname(silver_file_path), exist_ok=True)\n",
    "    df_silver.to_csv(silver_file_path, index=False)\n",
    "    print(f\"\\nDados limpos salvos: {df_silver.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bfc216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Crop  YEAR  Season     Area  Production  Yield   JAN   FEB   MAR  \\\n",
      "355     MAIZE  2007    RABI   230.81       879.0   3.81   1.7  36.7  35.2   \n",
      "356     MAIZE  2008  SUMMER   239.00       641.0   2.68  18.4  19.3  41.2   \n",
      "357     MAIZE  2009  SUMMER   163.00       405.0   2.48  12.0  12.0  14.2   \n",
      "358     MAIZE  2010    RABI     3.84        18.0   4.74   7.5  17.0  14.0   \n",
      "359     MAIZE  2011    RABI     4.00         3.0   0.68   6.8  25.8  22.4   \n",
      "...       ...   ...     ...      ...         ...    ...   ...   ...   ...   \n",
      "325748  WHEAT  2014    RABI  1622.00      3663.0   2.26  19.2  27.4  36.1   \n",
      "325749  WHEAT  2015    RABI   855.00      1241.0   1.45  17.9  25.6  36.1   \n",
      "325750  WHEAT  2016    RABI  1366.00      2415.0   1.77  17.6  26.2  34.4   \n",
      "325751  WHEAT  2017    RABI  1052.00      2145.0   2.04  18.2  26.6  33.0   \n",
      "325752  WHEAT  2018    RABI   833.00      2114.0   2.54  18.5  26.5  32.1   \n",
      "\n",
      "         APR  ...  Jun-Sep  Oct-Dec  TEMP_ANNUAL  TEMP_JAN_FEB  TEMP_MAR_MAY  \\\n",
      "355     30.6  ...    943.0     85.4        24.77         20.10         26.69   \n",
      "356     29.5  ...    877.8     88.1        24.61         19.16         26.46   \n",
      "357     25.1  ...    698.3    136.1        25.11         20.72         26.86   \n",
      "358     39.0  ...    911.1    153.2        25.13         20.19         27.83   \n",
      "359     41.1  ...    901.3     65.8        24.67         19.54         26.38   \n",
      "...      ...  ...      ...      ...          ...           ...           ...   \n",
      "325748  22.2  ...    782.1     85.3        24.73         19.58         26.24   \n",
      "325749  42.1  ...    878.5    121.6        24.91         20.12         26.29   \n",
      "325750  42.8  ...    886.6    125.0        25.15         22.25         28.86   \n",
      "325751  41.5  ...    897.7    126.0        24.79         21.84         28.41   \n",
      "325752  39.5  ...    889.8    126.8        25.09         21.24         28.16   \n",
      "\n",
      "        TEMP_JUN_SEP  TEMP_OCT_DEC               data_ingestao  \\\n",
      "355            27.49         22.32  2025-11-17 13:44:22.968398   \n",
      "356            27.26         22.87  2025-11-17 13:44:22.968398   \n",
      "357            27.89         22.58  2025-11-17 13:44:22.968398   \n",
      "358            27.50         22.60  2025-11-17 13:44:22.968398   \n",
      "359            27.54         22.71  2025-11-17 13:44:22.968398   \n",
      "...              ...           ...                         ...   \n",
      "325748         27.88         22.47  2025-11-17 13:44:22.968398   \n",
      "325749         27.73         22.99  2025-11-17 13:44:22.968398   \n",
      "325750         28.44         24.20  2025-11-17 13:44:22.968398   \n",
      "325751         28.50         24.21  2025-11-17 13:44:22.968398   \n",
      "325752         28.28         23.55  2025-11-17 13:44:22.968398   \n",
      "\n",
      "                               State District         data_processamento  \n",
      "355     ANDAMAN AND NICOBAR ISLAND - NICOBARS 2025-11-17 13:44:38.349525  \n",
      "356     ANDAMAN AND NICOBAR ISLAND - NICOBARS 2025-11-17 13:44:38.349525  \n",
      "357     ANDAMAN AND NICOBAR ISLAND - NICOBARS 2025-11-17 13:44:38.349525  \n",
      "358     ANDAMAN AND NICOBAR ISLAND - NICOBARS 2025-11-17 13:44:38.349525  \n",
      "359     ANDAMAN AND NICOBAR ISLAND - NICOBARS 2025-11-17 13:44:38.349525  \n",
      "...                                       ...                        ...  \n",
      "325748                  WEST BENGAL - PURULIA 2025-11-17 13:44:38.349525  \n",
      "325749                  WEST BENGAL - PURULIA 2025-11-17 13:44:38.349525  \n",
      "325750                  WEST BENGAL - PURULIA 2025-11-17 13:44:38.349525  \n",
      "325751                  WEST BENGAL - PURULIA 2025-11-17 13:44:38.349525  \n",
      "325752                  WEST BENGAL - PURULIA 2025-11-17 13:44:38.349525  \n",
      "\n",
      "[55682 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_silver)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
